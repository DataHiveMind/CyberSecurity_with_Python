{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Machine Learning for Cybersecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:\\Users\\kenne\\OneDrive\\Desktop\\Data Science-Development\\DataScienceBootcamp\\north_korea_missile_test_database.csv')\n",
    "y = df['Missile Name']\n",
    "x = df.drop(\"Missile Name\", axis = 1)\n",
    "\n",
    "\n",
    "xTrain, yTrain, xTest, yTest = train_test_split(x, y, test_size = 0.2, random_state = 31)\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size = 0.2, random_state = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data = pd.read_csv('C:\\Users\\kenne\\OneDrive\\Desktop\\Data Science-Development\\DataScienceBootcamp\\file_pe_headers.csv, sep = \",\"')\n",
    "x = data.drop(['Name', 'Malware'], axis = 1).to_numpy()\n",
    "xStand = StandardScaler().fit_transform(x)\n",
    "print(xStand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing lib and reading the dataset\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\Users\\kenne\\OneDrive\\Desktop\\Data Science-Development\\DataScienceBootcamp\\file_pe_headers.csv', sep = \",\")\n",
    "x = data.drop(['Name', 'Malware'], axis = 1).to_numpy()\n",
    "\n",
    "# Standardize the dataset before using PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_std = StandardScaler().fit_transform(x)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit_transform(x_std)\n",
    "print(pca.explained_variance_ratio_)\n",
    "sum(pca.explained_variance_ratio_[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a markov chain\n",
    "%pip install markovify\n",
    "import markovify\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "df = pd.read_csv('C:\\Users\\kenne\\OneDrive\\Desktop\\Data Science-Development\\DataScienceBootcamp\\airport_reviews.csv')\n",
    "n = 100\n",
    "review_subset = df['content'][0:n]\n",
    "text = \"\".join(chain.from_iterable(review_subset))\n",
    "mark = markovify.Text(text)\n",
    "\n",
    "for i in range(5):\n",
    "    print(mark.make_sentence())\n",
    "    \n",
    "for i in range(3):\n",
    "    print(mark.make_short_sentence(140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Clustering using scikit-learn\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.read_csv('C:\\Users\\kenne\\OneDrive\\Desktop\\Data Science-Development\\DataScienceBootcamp\\file_pe_headers.csv', sep = \",\")\n",
    "fig = px.scatter_3d(df, \n",
    "                    x = 'SuspiciousImportFunctions', \n",
    "                    y = 'SectionLength',\n",
    "                    z = \"SuspiciousNameSection\",  \n",
    "                    color = \" Malware\")\n",
    "fig.show()\n",
    "\n",
    "y = df[\"Malware\"]\n",
    "x = df.drop([\"Name\", \"Malware\"], axis = 1).to_numpy()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "est = KMeans(n_clusters= len(set(y)))\n",
    "est.fit(x)\n",
    "\n",
    "ypred = est.predict(x)\n",
    "df[\"pred\"] = ypred\n",
    "df[\"pred\"] = df[\"pred\"].astype(\"category\")\n",
    "\n",
    "fig = px.scatter_3d(df, \n",
    "                    x = 'SuspiciousImportFunctions', \n",
    "                    y = 'SectionLength',\n",
    "                    z = \"SuspiciousNameSection\",  \n",
    "                    color = \"pred\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training an XGBoost Classifier\n",
    "import pandas as pd\n",
    "df = pd.read_csv('C:\\Users\\kenne\\OneDrive\\Desktop\\Data Science-Development\\DataScienceBootcamp\\file_pe_headers.csv', sep = \",\")\n",
    "y = df[\"Malware\"]\n",
    "x = df.drop([\"Name\",\"Malware\"], axis=1).to_numpy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "XGB_model = XGBClassifier()\n",
    "XGB_model.fit(x_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test_pred = XGB_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy %.2f%%\" %(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Time Series using Statsmodels\n",
    "%pip installs statsmodels scipy\n",
    "from random import random\n",
    "time_series = [2*x+random() for i in range(1,100)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(time_series)\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "model = AR(time_series)\n",
    "model_fit = model.fit()\n",
    "y = model_fit.predict(len(time_series), len(time_series))\n",
    "\n",
    "from statsmodel.tsa.arima_model import ARIMA\n",
    "model = ARIMA(time_series, order = (0, 1))\n",
    "model_fit= model.fit(disp = False)\n",
    "y = model_fit.predict(len(time_series), len(time_series))\n",
    "print(y)\n",
    "\n",
    "from statsmodel.tsa.holwinters import SimpleExpSmoothing\n",
    "model = SimpleExpSmoothing(time_series)\n",
    "model_fit = model.fit()\n",
    "y = model_fit.predict(len(time_series), len(time_series))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection with Isolation Forest Implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random_seed = np.random.RandomState(12)\n",
    "x_train = 0.5*random_seed.randn(500,2)\n",
    "x_train = np.r_[x_train + 3, x_train]\n",
    "x_train = pd.DataFrame(x_train, columns = ['x', 'y'])\n",
    "\n",
    "x_test = 0.5*random_seed.randn(500,2)\n",
    "x_test = np.r_[x_test + 3, x_test]\n",
    "x_test = pd.DataFrame(x_test, columns = ['x', 'y'])\n",
    "\n",
    "x_outliers = random_seed.uniform(low=-5, high=5, size=(50,2))\n",
    "x_outliers = pd.DataFrame(x_outliers, columns = ['x', 'y'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "p1 = plt.scatter(x_train['x'], x_train['y'], c = 'white', s = 50, edgecolor = \"black\")\n",
    "p2 = plt.scatter(x_train['x'], x_train['y'], c = 'green', s = 50, edgecolor = \"black\")\n",
    "p3 = plt.scatter(x_train['x'], x_train['y'], c = 'blue', s = 50, edgecolor = \"black\")\n",
    "\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.legend(\n",
    "    [p1, p2, p3],\n",
    "    ['Training set', 'Normal Testing set', 'Anomalous Testing Data'],\n",
    "    loc = 'lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest()\n",
    "y_pred_train = clf.predict(x_train)\n",
    "y_pred_test = clf.predict(x_test)\n",
    "y_pred_outliers = clf.predict(x_outliers)\n",
    "\n",
    "x_outliers = x_outliers.assign(pred = y_pred_outliers)\n",
    "x_outliers.head()\n",
    "\n",
    "p1 = plt.scatter(x_train['x'], x_train['y'], c = 'white', s = 50, edgecolor = \"black\")\n",
    "p2 = plt.scatter(x_outliers.loc[x_outliers['pred'] == -1, 'x'], \n",
    "                 x_outliers.loc[x_outliers['pred'] == -1, 'y'],\n",
    "                 c = \"Blue\", s = 50, edgecolor = \"black\")\n",
    "\n",
    "p3 = plt.scatter(x_outliers.loc[x_outliers['pred'] == 1, 'x'], \n",
    "                 x_outliers.loc[x_outliers['pred'] == 1, 'y'],\n",
    "                 c = \"red\", s = 50, edgecolor = \"black\")\n",
    "\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.legend(\n",
    "    [p1, p2, p3],\n",
    "    ['Training obervations', 'Detected outliners', 'Incorrectly labled outliners'],\n",
    "    loc = 'lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "x_test = x_test.assign(pred = y_pred_test)\n",
    "x_test.head()\n",
    "\n",
    "\n",
    "p1 = plt.scatter(x_train['x'], x_train['y'], c = 'white', s = 50, edgecolor = \"black\")\n",
    "p2 = plt.scatter(x_test.loc[x_test['pred'] == -1, 'x'], \n",
    "                 x_test.loc[x_test['pred'] == -1, 'y'],\n",
    "                 c = \"Blue\", s = 50, edgecolor = \"black\")\n",
    "\n",
    "p3 = plt.scatter(x_test.loc[x_test['pred'] == 1, 'x'], \n",
    "                 x_test.loc[x_test['pred'] == 1, 'y'],\n",
    "                 c = \"red\", s = 50, edgecolor = \"black\")\n",
    "\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.legend(\n",
    "    [p1, p2, p3],\n",
    "    ['Training obervations', 'Detected outliners', 'Incorrectly labled outliners'],\n",
    "    loc = 'lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing using a Hasing Vectorizer and tf-idf Introduction\n",
    "with open(\"anonops_short.txt\", encoding = \"utf8\") as f:\n",
    "    anonops_short = f.readlines()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "my_vector = CountVectorizer(input = \"content\", ngram_range = (1, 2)) \n",
    "x_train_counts = my_vector.fit_transform(anonops_short)\n",
    "tf_transfomer = TfidfTransformer(use_idf = True,).fit(x_train_counts)\n",
    "x_train_tf = tf_transfomer.transform(x_train_counts)   \n",
    "\n",
    "print(x_train_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detetcing Email Cybersecurity Threats with AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Perception-based spam filter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('spam.csv')\n",
    "y = df.iloc[:, 0].values\n",
    "y = np.where(y == 'spam', -1, 1)\n",
    "x = df.iloc[:, 1:2].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "p = Perceptron(max_iter = 40, eta = 0.1, random_state = 0)\n",
    "p.fit(x_train, y_train)\n",
    "\n",
    "y_pred = p.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Misclassified samlpes: %d\", (y_test != y_pred)).sum()\n",
    "print(\"Accuracy: %.2f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam Detection with SVMS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('spam.csv')\n",
    "y = df.iloc[:, 0].values\n",
    "y = np.where(y == 'spam', -1, 1)\n",
    "x = df.iloc[:, 1:2].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "p = Perceptron(max_iter = 40, eta = 0.1, random_state = 0)\n",
    "p.fit(x_train, y_train)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel = 'linear', c = 1.0,random_state = 0)\n",
    "svm.fit(x_train, y_train)\n",
    "y_pred = svm.predict(x_test)\n",
    "\n",
    "from defs import plot_decision_regions\n",
    "x_combined = np.vstack((x_train, x_test))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "plot_decision_regions(x_combined, y_combined, classifier = svm, test_idx = range(-15,15))\n",
    "plt.xlabel('Suspect words')\n",
    "plt.ylabel('Spam or ham')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Misclassified samples: %d\" % (y_test!= y_pred).sum())\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression for spam detection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('spam.csv')\n",
    "x = df.iloc[:, 1:2].values\n",
    "y = df.iloc[:, 0].values\n",
    "y = np.where(y == 'spam', -1, 1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear = LinearRegression()\n",
    "linear.fit(x, y)\n",
    "print(linear.score(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression implementation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "phishing = np.genfromtxt('phishing.csv', delimiter=',', type = np.int(32))\n",
    "samples = phishing[:,:-1]\n",
    "targets = phishing[:-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, test_samples, train_targets, test_targets = train_test_split(samples, targets, test_size = 0.2, random_state = 0)\n",
    "log_class = LogisticRegression()\n",
    "log_class.fit(train_samples, train_targets)\n",
    "predictions = log_class.predict(test_samples)\n",
    "accuracy = 100.0* accuracy_score(test_targets, predictions)\n",
    "print(\"Logistic Regression Accuracy: \" +  str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phishing Detection with Decision Tress\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "phishing = np.genfromtxt('phishing.csv', delimiter=',', dtype = np.int(32))\n",
    "samples = phishing[:,:-1]\n",
    "targets = phishing[:-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, test_samples, train_targets, test_targets = train_test_split(samples, targets, test_size = 0.2, random_state= 0)\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf = tree_clf.fit(train_samples, train_targets)\n",
    "predictions = tree_clf.predict(test_samples)\n",
    "accuracy = 100.0* accuracy_score(test_targets, predictions)\n",
    "print(\"Decision Tree accurcay\" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing with Naive Bayes Classifier\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from defs import get_tokens\n",
    "from defs import get_lemmas\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sms = pd.read_csv(\"spam_no_header.csv\", sep = \",\", names = [\"type\", \"text\"])\n",
    "text_train, text_test = train_test_split(sms[\"text\"], sms['type'],test_size = 0.3)\n",
    "bow = CountVectorizer(analyzer = get_lemmas).fit(text_train)\n",
    "sms_bow = bow.transform(text_train)\n",
    "tfidf = TfidfTransformer().fit(sms_bow)\n",
    "sms_tfidf = tfidf.transform(sms_bow)\n",
    "spam_detector = MultinomialNB().fit(sms_tfidf, type_train)\n",
    "\n",
    "mgs = sms[\"text\"][25]\n",
    "mgs_bow = bow.transform([mgs])\n",
    "mgs_tfidf = tfidf.transform([mgs_bow])\n",
    "print(\"Predicted:\" , spam_detector.predict(mgs_tfidf)[0])\n",
    "print(\"Expected:\", sms.type[25])\n",
    "\n",
    "predictions = spam_detector.predict(sms_tfidf)\n",
    "print(\"Accuracy: \", accuracy_score(sms['type'][:len(predictions)], predictions))\n",
    "print(classification_report(sms['type'][:len(predictions)], predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malware Threat Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malware Detection with Decision Tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "malware_data = pd.read_csv('malware_data.csv', delimiter = \",\")\n",
    "samples = malware_data.iloc[:[0,4]].values\n",
    "target = malware_data.iloc[:,8].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, test_samples = train_test_split(samples, target,test_size = 0.2)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_class = DecisionTreeClassifier()\n",
    "tree_class.fit(train_samples, target)\n",
    "predictions = tree_class.predict(test_samples)\n",
    "accurcay = 100* accuracy_score(test_samples, predictions)\n",
    "print(\"Decision Tree Accuracy: \"+ str(accurcay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malware Detection with Random Forest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "malware_data = pd.read_csv('malware_data.csv', delimiter = \",\")\n",
    "samples = malware_data.iloc[:[0,4]].values\n",
    "target = malware_data.iloc[:,8].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, test_samples, train_targets, test_targets = train_test_split(samples, targets, test_size = 0.2, random_state= 0)\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators = 100)\n",
    "rfc.fit(train_samples, train_targets)\n",
    "accuracy = rfc.score(test_samples, train_targets)\n",
    "print(\"Random Forest Classifier Accuracy: \" + str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malware Detection with K-Means\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "malware_data = pd.read_csv('malware_data.csv', delimiter = \",\")\n",
    "samples = malware_data.iloc[:, [1,2,3,4]].values\n",
    "targets = malware_data.iloc[:, 8].values\n",
    "k_means = KMeans(n_clusters = 3, max_iter= 300)\n",
    "k_means.fit(samples)\n",
    "\n",
    "print(\"k_Means labels: \", str(k_means.labels_))\n",
    "print(\"K-means clustering results \", pd.crosstab(targets, k_means.labels_, rownames = [\"Oberverd\"], colnames = [\"Predicted\"]))\n",
    "print(\"Silhouette Coefficient: %0.3f\"% silhouette_score(samples, k_means.labels_, metric = \"euclidean\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Malware Threat Detection with AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting obfuscated Javascript file\n",
    "import os \n",
    "from sklearn.feature_extraction.text import MashingVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "js_path = \"JavascriptSamples\"\n",
    "obfuscated_js_path = \"ObfuscatedJavascriptSamples\"\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "file_types_and_labels = [(js_path, 0), (obfuscated_js_path, 1)]\n",
    "\n",
    "for files_path, label in file_types_and_labels:\n",
    "    files = os.listdir(files_path)\n",
    "    for file in files:\n",
    "        file_path = file_path + \"/\" + file\n",
    "        try:\n",
    "            with open(file_path, \"r\") as myfile:\n",
    "                data = myfile.read().replace(\"\\n\", \"\")\n",
    "                data = str(data)\n",
    "                corpus.append(data)\n",
    "                labels.append(label)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(corpus, labels, test_size = 0.3, random_state = 42)\n",
    "text_clf =  Pipeline([('vect', MashingVector(input = \"content\"), ngram_range = (1,3),\"tfidf\", TfidfTransformer(use_idf = True),\"rf\", RandomForestClassifier(class_weight = \"balanced\"))])\n",
    "\n",
    "text_clf.fit(x_train, y_train)\n",
    "y_test_pred = text_clf.predict(x_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_test_pred))\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Malware drift\n",
    "month0 = {\"Trojan\":24, \"CryptoMiner\": 11, \"other\":36, \"Worm\": 29}\n",
    "month1 = {\"Trojan\":28, \"CryptoMiner\": 25, \"other\":22, \"Worm\": 25}\n",
    "month2 = {\"Trojan\":18, \"CryptoMiner\": 36, \"other\":41, \"Worm\": 5}\n",
    "month3 = {\"CryptoMiner\":24, \"Trojan\": 33, \"other\":44, \"Worm\": 5}\n",
    "months = [month0, month1, month2, month3]\n",
    "\n",
    "trojan_time_series = []\n",
    "crypto_miner_time_series = []\n",
    "worm_time_series = []\n",
    "other_time_series = []\n",
    "\n",
    "for month in months:\n",
    "    trojan_time_series.append(month[\"Trojan\"])\n",
    "    crypto_miner_time_series.append(month[\"CryptoMiner\"])\n",
    "    worm_time_series.append(month[\"Worm\"])\n",
    "    other_time_series.append(month[\"other\"])\n",
    "\n",
    "plt.title(\"Trojan\")\n",
    "plt.plot(trojan_time_series, label = \"Trojan\")\n",
    "plt.savefig(\"TrojanGraph.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Crypto_miner\")\n",
    "plt.plot(trojan_time_series, label = \"Crypto_miner\")\n",
    "plt.savefig(\"CryptoMinerGraph.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Worm\")\n",
    "plt.plot(trojan_time_series, label = \"Worm\")\n",
    "plt.savefig(\"WormGraph.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Other\")\n",
    "plt.plot(trojan_time_series, label = \"Other\")\n",
    "plt.savefig(\"OtherGraph.png\")\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "ts_model = ARIMA(trojan_time_series, order = (0,1))\n",
    "model_fit_to_data = ts_model.fit(disp = True)\n",
    "y_Trojan = model_fit_to_data.predict(len(trojan_time_series), len(trojan_time_series))\n",
    "print(\"Predicted Trojan count for next month: \", str(y_Trojan[0]), + \"%\")\n",
    "\n",
    "ts_model = ARIMA(crypto_miner_time_series, order = (0,1))\n",
    "model_fit_to_data = ts_model.fit(disp = True)\n",
    "y_Trojan = model_fit_to_data.predict(len(crypto_miner_time_series), len(crypto_miner_time_series))\n",
    "print(\"Predicted Trojan count for next month: \", str(y_Trojan[0]), + \"%\")\n",
    "\n",
    "ts_model = ARIMA(worm_time_series, order = (0,1))\n",
    "model_fit_to_data = ts_model.fit(disp = True)\n",
    "y_Trojan = model_fit_to_data.predict(len(worm_time_series), len(worm_time_series))\n",
    "print(\"Predicted Trojan count for next month: \", str(y_Trojan[0]), + \"%\")\n",
    "\n",
    "ts_model = ARIMA(other_time_series, order = (0,1))\n",
    "model_fit_to_data = ts_model.fit(disp = True)\n",
    "y_Trojan = model_fit_to_data.predict(len(other_time_series), len(other_time_series))\n",
    "print(\"Predicted Trojan count for next month: \", str(y_Trojan[0]), + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Botnet Detection with Machine Learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = pd.read_csv(\"botnet_data.csv\")\n",
    "samples = dataset.iloc[:, [1,2]].values\n",
    "target = dataset.iloc[\"ANOMALY\"].values\n",
    "training_samples, testing_samples, training_target, testing_target = train_test_split(samples, target, test_size = 0.3, random_state = 0)\n",
    "\n",
    "knc = KNeighborsClassifier(n_neighbors = 2)\n",
    "knc.fit(training_samples, training_target)\n",
    "knc_pred = knc.predict(testing_samples)\n",
    "knc_accuracy = 100.0 * accuracy_score(testing_target, knc_pred)\n",
    "print(\"KNeighborsClassifier accuracy: \" + str(knc_accuracy))\n",
    "\n",
    "dfc = DecisionTreeClassifier(random_state=0)\n",
    "dfc.fit(training_samples, training_target)\n",
    "dfc_pred = dfc.predict(testing_samples)\n",
    "dfc_accuracy = 100.0 * accuracy_score(testing_target, dfc_pred)\n",
    "print(\"DecisionTreeClassifier accuracy: \" + str(dfc_accuracy))\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(training_samples, training_target)\n",
    "gnb_pred = gnb.predict(testing_samples)\n",
    "gnb_accuracy = 100.0 * accuracy_score(testing_target, gnb_pred)\n",
    "print(\"GaussianNB accuracy: \" + str(gnb_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection Module\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class AnomalyDetector:\n",
    "    def __init__(self, data):\n",
    "        (self.mu_param, self.sigma_squared) = GaussionAnomalyDetection.estimate_gaussian(data)\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "    def mulitvarible_gaussion(self, data):\n",
    "        mu_param = self.mu_param\n",
    "        sigma_squared = self.sigma_squared\n",
    "        (num_examples, num_features) = data.shape\n",
    "        probabilities = np.zeros((num_examples, 1))\n",
    "        \n",
    "        for example_index in range(num_examples):\n",
    "            for feature_index in range(num_features):\n",
    "                power_divdend = (data[example_index, feature_index] - mu_param[feature_index]) ** 2\n",
    "                power_divder = 2 * sigma_squared[feature_index]\n",
    "                e_power = -1 * power_divdend / power_divder\n",
    "                \n",
    "                probabilities_prefix = 1/math.sqrt(2 * math.pi * sigma_squared[feature_index])\n",
    "                probability = probabilities_prefix * math.exp(e_power)\n",
    "                probabilities[example_index] *= probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def estimate_gaussian(self, data):\n",
    "        num_examples = data.shape[0]\n",
    "        mu_param = (1 / num_examples) * np.sum(data, axis = 0)\n",
    "        sigma_squared = (1 / num_examples) * np.sum((data - mu_param) ** 2, axis = 0)\n",
    "        \n",
    "        return (mu_param, sigma_squared)\n",
    "    \n",
    "    def select_threshold(labels, probabilities):\n",
    "        best_epsilon = 0\n",
    "        best_fl = 0\n",
    "        \n",
    "        precision_history = []\n",
    "        recall_history = []\n",
    "        fl_history = []\n",
    "        \n",
    "        min_probability = np.min(probabilities)\n",
    "        max_probability = np.max(probabilities)\n",
    "        step_size = (max_probability - min_probability) / 1000\n",
    "        \n",
    "        for epsilon in np.arange(min_probability, max_probability, step_size):\n",
    "            predictions = probabilities < epsilon\n",
    "            false_positives = np.sum((predictions == 1) & (labels == 0))\n",
    "            false_negatives = np.sum((predictions == 0) & (labels == 1))\n",
    "            true_positives = np.sum((predictions == 1) & (labels == 1))\n",
    "            \n",
    "            if (true_positives + false_positives) == 0 or (true_positives + false_negatives) == 0:\n",
    "                continue\n",
    "            \n",
    "            precision = true_positives / (true_positives + false_positives)\n",
    "            recall = true_positives / (true_positives + false_negatives)\n",
    "            fl_score = 2 * precision * recall / (precision + recall)\n",
    "                \n",
    "            if fl_score  > best_fl:\n",
    "                best_epsilon = epsilon\n",
    "                best_fl = fl_score   \n",
    "                \n",
    "        return best_epsilon, best_fl, precision_history, recall_history, fl_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussion Anomaly Detection Implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "hist_dist = dataset[[\"LATENCY\", \"THROUGHPUT\"]].hist(grid = False, figsize = (10, 4))\n",
    "\n",
    "data = dataset[[\"LATENCY\", \"THROUGHPUT\"]].values\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha = 0.6)\n",
    "plt.xlabel(\"Latency (ms)\")\n",
    "plt.ylabel(\"THROUGHPUT\")\n",
    "plt.title(\"Date Flow\")\n",
    "plt.show()\n",
    "\n",
    "from guassian_anomaly_detection import GuassianAnomalyDetection\n",
    "gaussian_anomaly_detection = GuassianAnomalyDetection(data)\n",
    "print(\"mu param estimation: \")\n",
    "print(gaussian_anomaly_detection.mu_param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Securing Users Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keystroke Demonstration \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pwd_data = pd.read_csv(\"pwd_data.csv\", header = 0)\n",
    "DD = [dd for dd in pwd_data.columns if dd.startswuth(\"DD\")]\n",
    "plot = pwd_data[DD]\n",
    "plot[\"subject\"] = pwd_data[\"subject\"].Values\n",
    "plot = plot.groupby(\"subject\").mean()\n",
    "plot.iloc[:6].T.plot(figsize = (8,6), title = \"Average Keystroke Latency per Subjects\")\n",
    "\n",
    "data_train, data_test = train_test_split(pwd_data, test_size = 0.2, random_state = 0)\n",
    "x_train = data_train[pwd_data.columns[2:]]\n",
    "y_train = data_train[\"subject\"]\n",
    "x_test = data_test[pwd_data.columns[2:]]\n",
    "y_test = data_test[\"subject\"]\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "knc.fit(x_train, y_train)\n",
    "y_pred = knc.predict(x_test)\n",
    "knc_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"KNeighborsClassifier Accuracy: \" + str(knc_accuracy))\n",
    "\n",
    "svc = svm.SVC(kernal = \"linear\")\n",
    "svc.fit(x_train, y_train)\n",
    "y_pred = svc.predict(x_test)\n",
    "svc_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"SVC Accuracy: \" + str(svc_accuracy))\n",
    "\n",
    "mlpc = MLPClassifier()\n",
    "mlpc.fit(x_train, y_train)\n",
    "y_pred = mlpc.predict(x_test)\n",
    "mlpc_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"MLPClassifier Accuracy: \" + str(mlpc_accuracy))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "labels = list(pwd_data['subject'].unique())\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "figure = plt.figure()\n",
    "axes = figure.add_subplot(111)\n",
    "figure.colorvar(axes.matshow(cm))\n",
    "axes.set_xticklabels([''] + labels)\n",
    "axes.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elgenfaces Implementation\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lfw = fetch_lfw_people(min_faces_per_person = 100)\n",
    "x_data = lfw.data\n",
    "y_target = lfw.target\n",
    "names = lfw.target_names\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_target, test_size = 0.3)\n",
    "\n",
    "pca = PCA(n_components = 150, whiten = True)\n",
    "pca.fit(x_train)\n",
    "pac_train = pca.transform(x_train)\n",
    "pca_test = pca.transform(x_test)\n",
    "mlpc = MLPClassifier()\n",
    "mlpc.fit(pac_train, y_train)\n",
    "y_pred = mlpc.predict(pca_test)\n",
    "print(classification_report(y_test, y_pred, target_names = names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Intrusion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Card Fraud Detection Implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fraud_df = pd.read_csv(\"FinancialFraudDB.csv\", index_col = None)\n",
    "card_replacement_cost = 5\n",
    "customer_freeze_cost = 3\n",
    "\n",
    "cost_matrix = np.zeros(len(fraud_df.index), 4)\n",
    "cost_matrix[:, 0] = card_replacement_cost * np.ones(len(fraud_df.index))\n",
    "cost_matrix[:, 1] = fraud_df[\"Amount\"].values\n",
    "cost_matrix[:, 2] = card_replacement_cost * np.ones(len(fraud_df.index))\n",
    "\n",
    "y = fraud_df.pop(\"Class\").values\n",
    "x = fraud_df.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sets = train_test_split(x, y, cost_matrix, test_size = 0.25, random_state = 11)\n",
    "x_train, x_test, y_train, y_test, cost_matrix_train, cost_matrix_test = sets\n",
    "\n",
    "from sklearn import tree\n",
    "y_pred_test_dt = tree.DecisionTreeClassifier().fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "%pip install costcla\n",
    "from costcla.models import CostSenitiveDecisionTreeClassifier\n",
    "y_pred_test_csdt = CostSenitiveDecisionTreeClassifier().fit(x_train, y_train, cost_matrix_train).predict(x_test)\n",
    "\n",
    "from costcla.metrics import savings_score\n",
    "print(savings_score(y_test, y_pred_test_dt, cost_matrix_test))\n",
    "print(savings_score(y_test, y_pred_test_csdt, cost_matrix_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counterfeit Banknote Detection Implementation\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data_banknote_authentication.txt\", header = None)\n",
    "df.columns = [\"0\", \"1\", \"2\", \"3\", \"label\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df)\n",
    "\n",
    "y_train = df_train.pop(\"label\").values\n",
    "x_train = df_train.values\n",
    "y_test = df_test.pop(\"label\").values\n",
    "x_test = df_test.values\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad Blocking using Machine Learning Implementation\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"ad.data\", header = None)\n",
    "df.rename(columns = {1558: \"label\"}, inplace = True)\n",
    "\n",
    "improper_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    for col in df.columns:\n",
    "        val = str(row[col]).strip()\n",
    "        if val == \"?\":\n",
    "            improper_rows.append(index)\n",
    "            \n",
    "df = df.drop(df.index[list(set(improper_rows))])\n",
    "def label_to_numeric(row):\n",
    "    if row['label'] == \"ad\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df[\"label\"] = df.apply(label_to_numeric, axis = 1) \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df)\n",
    "y_train = df_train.pop(\"label\").values\n",
    "y_test = df_test.pop(\"label\").values\n",
    "x_train =  df_train.values\n",
    "x_test = df_test.values\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wireless Indoor Localization Implemention\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"wifi_localization.txt\", sep = \"\\t\",header = None)\n",
    "df = df.rename(columns = {7: \"room\"})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df)\n",
    "y_train = df_train.pop(\"room\").values\n",
    "y_test = df_test.pop(\"room\").values\n",
    "x_train = df_train.values\n",
    "x_test = df_test.values\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix    \n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOT Device Type Identification using Machine Learning Implementation\n",
    "import pandas as pd\n",
    "import os\n",
    "training_data = pd.read_csv(\"iot_device_type_training_data.csv\")\n",
    "testing_data = pd.read_csv(\"iot_device_type_test_data.csv\")\n",
    "\n",
    "x_train, y_train = (training_data.loc[: , training_data.columns != \"device_type\"].values,\n",
    "                    training_data[\"device_category\"])\n",
    "\n",
    "x_test, y_test = (testing_data.loc[: , testing_data.columns != \"device_type\"].values,\n",
    "                    testing_data[\"device_category\"])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(training_data[\"device_category\"].unique())\n",
    "y_train_encoded = le.transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train_encoded)\n",
    "model.score(x_test, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepfake Recognition using Machine Learning Implementation\n",
    "from mesonet_classifiers import *\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "MesoNet_classifier = Meso4()\n",
    "MesoNet_classifier.load(\"weights/Meso4_DF\")\n",
    "\n",
    "image_data_generator = ImageDataGenerator(rescale = 1./255)\n",
    "data_generator = image_data_generator.flow_from_directory(\"\",classes = [\"mesonet_test_images\"])\n",
    "\n",
    "nue_to_label = {1: \"real\", 0: \"fake\"}\n",
    "x,y = data_generator.next()\n",
    "probilities = MesoNet_classifier.predict(x)\n",
    "predictions = [nue_to_label[round(x[0])] for x in probilities]\n",
    "print(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Securing and Attacking Data with Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing Password Security using Machine Learning Implementation\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"passwordDataset.csv\", dtype = {\"password\": str, \"strength\": int}, index_col = None)\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "train_df = df.head(int(1 * 0.8))\n",
    "test_df = df.tail(int(1 * 0.2))\n",
    "\n",
    "y_train = train_df.pop(\"strength\").values\n",
    "y_test = test_df.pop(\"strength\").values\n",
    "x_train = train_df.values.flatten()\n",
    "x_test = test_df.values.flatten()\n",
    "\n",
    "def charater_tokens(input_string):\n",
    "    return [x for x in input_string]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "password_clf = Pipeline([(\"vect\", TfidfVectorizer(tokenizer = charater_tokens)), (\"clf\", XGBClassifier()),])\n",
    "\n",
    "password_clf.fit(x_train, y_train)\n",
    "password_clf.score(x_test, y_test)\n",
    "\n",
    "common_password = \"qwerty\"\n",
    "strong_computer_generated_password = \"a1b2c3d4e5f6g7h8i9j0\"\n",
    "\n",
    "password_clf.predict([common_password, strong_computer_generated_password])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning-based Steganalysis Implementation\n",
    "boss_features_path = \"boosbase.fea\"\n",
    "bossbase_lsb_features_path = \"bossbase_lsb.fea\"\n",
    "\n",
    "feature_with_labels = [(boss_features_path, 0), (bossbase_lsb_features_path, 0)]\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for feature_path, label in feature_with_labels:\n",
    "    with open(feature_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            fv = line.split()\n",
    "            x.append(fv)\n",
    "            y.append(label)\n",
    "            \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 11)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(x_train, y_train)\n",
    "print(clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIPAA Data Breaches - Data Exploration and Visualization\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"HIPAA-Breach-report-2000-to-2017.csv\")\n",
    "df = df.dropna()\n",
    "df.head()\n",
    "\n",
    "%matplotlib inline\n",
    "def_fig_size = (15, 6)\n",
    "df[\"Individuals Affected\"].plot(kind = \"hist\", \n",
    "                                figsize = def_fig_size,\n",
    "                                log = True,\n",
    "                                title = \"Distribution of Individuals Affected\")\n",
    "\n",
    "df.groupby(\"Coverd Entity Type\").mean().plot(kind = \"bar\", \n",
    "                                figsize = def_fig_size,\n",
    "                                log = True,\n",
    "                                title = \"Average Breach Size by Entity Type\")\n",
    "\n",
    "df.groupby(\"State\").sum().nlargest(20, \"Individuals Affected\").plot.pie(y = \"Individuals Affected\",\n",
    "                                                                        firgsize = def_fig_size,\n",
    "                                                                        legend = False)\n",
    "\n",
    "df.groupby(\"Type of Breach\").mean().plot(kind = \"bar\", \n",
    "                                figsize = def_fig_size,\n",
    "                                log = True,\n",
    "                                title = \"Average Breach Size by Type of Breach\")\n",
    "\n",
    "from sklearn.feature_extraction.text import tdidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "df[\"Web Description\"] = df[\"Web Description\"].str.replace(\"\\r\", \" \")\n",
    "x = df[\"web Description\"].values\n",
    "x_transformed = vect.fit_transform(x)                                                                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
